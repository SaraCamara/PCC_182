{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Instalações e bibliotecas necessárias"
      ],
      "metadata": {
        "id": "xRuR8DlsTD9h"
      }
    },
    {
      "source": [
        "!pip install transformers -U\n",
        "!pip install peft datasets torch peft\n",
        "!pip install imblearn"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsjCrCaLArMg",
        "outputId": "e898e18d-d4fc-4888-9fda-9cafc2a40f44",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: transformers in ./.local/lib/python3.10/site-packages (4.43.3)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.43.4-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: requests in ./.local/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./.local/lib/python3.10/site-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in ./.local/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./.local/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.10/site-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in ./.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.local/lib/python3.10/site-packages (from requests->transformers) (2.2.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.43.3\n",
            "    Uninstalling transformers-4.43.3:\n",
            "      Successfully uninstalled transformers-4.43.3\n",
            "Successfully installed transformers-4.43.4\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: peft in ./.local/lib/python3.10/site-packages (0.11.1)\n",
            "Requirement already satisfied: datasets in ./.local/lib/python3.10/site-packages (2.20.0)\n",
            "Requirement already satisfied: torch in ./.local/lib/python3.10/site-packages (2.3.1)\n",
            "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.10/site-packages (from peft) (1.26.4)\n",
            "Requirement already satisfied: safetensors in ./.local/lib/python3.10/site-packages (from peft) (0.4.3)\n",
            "Requirement already satisfied: transformers in ./.local/lib/python3.10/site-packages (from peft) (4.43.4)\n",
            "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from peft) (5.4.1)\n",
            "Requirement already satisfied: psutil in /usr/lib/python3/dist-packages (from peft) (5.9.0)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in ./.local/lib/python3.10/site-packages (from peft) (0.32.1)\n",
            "Requirement already satisfied: tqdm in ./.local/lib/python3.10/site-packages (from peft) (4.66.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in ./.local/lib/python3.10/site-packages (from peft) (0.23.4)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.10/site-packages (from peft) (24.1)\n",
            "Requirement already satisfied: xxhash in ./.local/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in ./.local/lib/python3.10/site-packages (from datasets) (2024.5.0)\n",
            "Requirement already satisfied: multiprocess in ./.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in ./.local/lib/python3.10/site-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: pandas in ./.local/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in ./.local/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: aiohttp in ./.local/lib/python3.10/site-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: pyarrow-hotfix in ./.local/lib/python3.10/site-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.local/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.local/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.local/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: triton==2.3.1 in ./.local/lib/python3.10/site-packages (from torch) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./.local/lib/python3.10/site-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.local/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.local/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: sympy in ./.local/lib/python3.10/site-packages (from torch) (1.13.0)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: jinja2 in ./.local/lib/python3.10/site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: networkx in ./.local/lib/python3.10/site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in ./.local/lib/python3.10/site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.local/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (2020.6.20)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.local/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: tzdata>=2022.7 in ./.local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./.local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./.local/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: imblearn in ./.local/lib/python3.10/site-packages (0.0)\n",
            "Requirement already satisfied: imbalanced-learn in ./.local/lib/python3.10/site-packages (from imblearn) (0.12.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.local/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (3.5.0)\n",
            "Requirement already satisfied: scipy>=1.5.0 in ./.local/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.14.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in ./.local/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.5.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in ./.local/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in ./.local/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModelForPreTraining\n",
        "\n",
        "# Carregar o modelo\n",
        "from transformers import BertForSequenceClassification"
      ],
      "metadata": {
        "id": "f_WcSYS8IDBb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c26e12ea-236b-43bb-ae29-f0b4a7bda095"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-08-05 09:41:53.457323: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-08-05 09:41:53.614499: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-08-05 09:41:53.680631: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-08-05 09:41:53.697871: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-08-05 09:41:53.813239: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-08-05 09:41:54.741460: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "7YMjQEbNxOfx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT"
      ],
      "metadata": {
        "id": "niB361IyfJCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Treino"
      ],
      "metadata": {
        "id": "od4mKkeh7ug_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset, load_metric\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "\n",
        "class BertWithPrefix(torch.nn.Module):\n",
        "    def __init__(self, bert_model, prefix_tokens):\n",
        "        super().__init__()\n",
        "        self.bert = bert_model\n",
        "        self.prefix_tokens = torch.nn.Parameter(prefix_tokens.clone().detach().float())\n",
        "        for name, param in self.bert.named_parameters():\n",
        "            if 'classifier' not in name:\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
        "        device = input_ids.device\n",
        "        batch_prefix_tokens = self.prefix_tokens.to(device).unsqueeze(0).repeat(input_ids.size(0), 1)\n",
        "        extended_input_ids = torch.cat([batch_prefix_tokens.long(), input_ids], dim=1)\n",
        "        if attention_mask is not None:\n",
        "            prefix_mask = torch.ones(batch_prefix_tokens.size(), dtype=torch.long, device=device)\n",
        "            extended_attention_mask = torch.cat([prefix_mask, attention_mask], dim=1)\n",
        "        else:\n",
        "            extended_attention_mask = None\n",
        "        return self.bert(input_ids=extended_input_ids, attention_mask=extended_attention_mask, labels=labels)\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\" Limpeza básica de texto \"\"\"\n",
        "    text = text.lower()  # minúsculas\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # remover URLs\n",
        "    text = re.sub(r'@\\w+', '', text)  # remover menções\n",
        "    text = re.sub(r'\\d+', '', text)  # remover números\n",
        "    text = re.sub(r'[^A-Za-záéíóúàèìòùâêîôûãõ\\b]', ' ', text)  # remover caracteres especiais\n",
        "    return text.strip()\n",
        "\n",
        "# Carregar e preparar dataset\n",
        "data = pd.read_csv('./repos/HEDOS/HEDOS.csv')\n",
        "data['text'] = data['text'].apply(clean_text)\n",
        "filtered_data = data[data['final_label'] != 'Lixo'].copy()\n",
        "filtered_data['final_label'] = filtered_data['final_label'].map({'not_toxic': 0, 'toxic': 1})\n",
        "filtered_data.dropna(subset=['text', 'final_label'], inplace=True)\n",
        "\n",
        "# Balanceamento de dados\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = ros.fit_resample(filtered_data[['text']], filtered_data['final_label'])\n",
        "resampled_data = pd.DataFrame({'text': X_resampled['text'], 'final_label': y_resampled})\n",
        "\n",
        "# Preparar tokenizer e modelo\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "# Define and prepare prefix\n",
        "prefix_text = \"Classifique como tóxico qualquer texto que contenha palavões!\"\n",
        "prefix_tokens = tokenizer(prefix_text, return_tensors=\"pt\", add_special_tokens=False)['input_ids'][0]\n",
        "\n",
        "def preprocess_function(examples, labels):\n",
        "    tokenized_inputs = tokenizer(examples, padding=\"max_length\", truncation=True, max_length=512-len(prefix_tokens))\n",
        "    return {'input_ids': tokenized_inputs['input_ids'], 'attention_mask': tokenized_inputs['attention_mask'], 'labels': labels}\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, predictions),\n",
        "        \"precision\": precision_score(labels, predictions, average='binary'),\n",
        "        \"recall\": recall_score(labels, predictions, average='binary'),\n",
        "        \"f1\": f1_score(labels, predictions, average='binary'),\n",
        "    }\n",
        "\n",
        "# Definindo K-Fold Cross Validation\n",
        "kf = StratifiedKFold(n_splits=5)\n",
        "results = []\n",
        "\n",
        "for train_index, test_index in kf.split(resampled_data['text'], resampled_data['final_label']):\n",
        "    train_data = resampled_data.iloc[train_index]\n",
        "    test_data = resampled_data.iloc[test_index]\n",
        "\n",
        "    train_encodings = preprocess_function(train_data['text'].tolist(), train_data['final_label'].tolist())\n",
        "    val_encodings = preprocess_function(test_data['text'].tolist(), test_data['final_label'].tolist())\n",
        "\n",
        "    train_dataset = Dataset.from_dict(train_encodings)\n",
        "    eval_dataset = Dataset.from_dict(val_encodings)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        evaluation_strategy='epoch',\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=8,\n",
        "        num_train_epochs=3,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=10,\n",
        "        fp16=True  # Usar GPU\n",
        "    )\n",
        "\n",
        "    bert_with_prefix = BertWithPrefix(model, prefix_tokens)\n",
        "    trainer = Trainer(\n",
        "        model=bert_with_prefix,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    train_result = trainer.train()\n",
        "    eval_result = trainer.evaluate()\n",
        "    results.append(eval_result)\n",
        "\n",
        "# Consolidar resultados\n",
        "average_results = {key: np.mean([dic[key] for dic in results]) for key in results[0]}\n",
        "print(\"Média dos resultados da validação cruzada:\", average_results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tb35pOEOfG51",
        "outputId": "a30e1a43-1035-4499-863d-f92ff2f408d2",
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/home/sarapinheiro/.local/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='645' max='645' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [645/645 1:59:59, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.684500</td>\n",
              "      <td>0.671947</td>\n",
              "      <td>0.603138</td>\n",
              "      <td>0.602064</td>\n",
              "      <td>0.609756</td>\n",
              "      <td>0.605886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.686600</td>\n",
              "      <td>0.671656</td>\n",
              "      <td>0.599070</td>\n",
              "      <td>0.595960</td>\n",
              "      <td>0.616725</td>\n",
              "      <td>0.606164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.684400</td>\n",
              "      <td>0.672208</td>\n",
              "      <td>0.599070</td>\n",
              "      <td>0.589155</td>\n",
              "      <td>0.656214</td>\n",
              "      <td>0.620879</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='216' max='216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [216/216 07:54]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/sarapinheiro/.local/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='645' max='645' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [645/645 2:00:10, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.683300</td>\n",
              "      <td>0.674301</td>\n",
              "      <td>0.576990</td>\n",
              "      <td>0.565217</td>\n",
              "      <td>0.665116</td>\n",
              "      <td>0.611111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.674100</td>\n",
              "      <td>0.675113</td>\n",
              "      <td>0.572342</td>\n",
              "      <td>0.557090</td>\n",
              "      <td>0.703488</td>\n",
              "      <td>0.621788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.678000</td>\n",
              "      <td>0.674047</td>\n",
              "      <td>0.578152</td>\n",
              "      <td>0.567404</td>\n",
              "      <td>0.655814</td>\n",
              "      <td>0.608414</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='216' max='216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [216/216 07:54]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/sarapinheiro/.local/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='645' max='645' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [645/645 2:00:03, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.663500</td>\n",
              "      <td>0.675204</td>\n",
              "      <td>0.584302</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.674419</td>\n",
              "      <td>0.618667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.671400</td>\n",
              "      <td>0.674249</td>\n",
              "      <td>0.583140</td>\n",
              "      <td>0.578142</td>\n",
              "      <td>0.615116</td>\n",
              "      <td>0.596056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.670100</td>\n",
              "      <td>0.674128</td>\n",
              "      <td>0.580233</td>\n",
              "      <td>0.578054</td>\n",
              "      <td>0.594186</td>\n",
              "      <td>0.586009</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='215' max='215' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [215/215 07:54]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/sarapinheiro/.local/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='645' max='645' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [645/645 2:00:01, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.668700</td>\n",
              "      <td>0.674750</td>\n",
              "      <td>0.587791</td>\n",
              "      <td>0.578728</td>\n",
              "      <td>0.645349</td>\n",
              "      <td>0.610225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.671400</td>\n",
              "      <td>0.674648</td>\n",
              "      <td>0.585465</td>\n",
              "      <td>0.586572</td>\n",
              "      <td>0.579070</td>\n",
              "      <td>0.582797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.689900</td>\n",
              "      <td>0.674453</td>\n",
              "      <td>0.588372</td>\n",
              "      <td>0.587356</td>\n",
              "      <td>0.594186</td>\n",
              "      <td>0.590751</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='215' max='215' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [215/215 07:54]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/sarapinheiro/.local/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='311' max='645' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [311/645 54:04 < 58:26, 0.10 it/s, Epoch 1.44/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.688600</td>\n",
              "      <td>0.651336</td>\n",
              "      <td>0.630233</td>\n",
              "      <td>0.614053</td>\n",
              "      <td>0.701163</td>\n",
              "      <td>0.654723</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 110\u001b[0m\n\u001b[1;32m    100\u001b[0m bert_with_prefix \u001b[38;5;241m=\u001b[39m BertWithPrefix(model, prefix_tokens)\n\u001b[1;32m    101\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m    102\u001b[0m     model\u001b[38;5;241m=\u001b[39mbert_with_prefix,\n\u001b[1;32m    103\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[1;32m    108\u001b[0m )\n\u001b[0;32m--> 110\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m eval_result \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[1;32m    112\u001b[0m results\u001b[38;5;241m.\u001b[39mappend(eval_result)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2284\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2285\u001b[0m ):\n\u001b[1;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2288\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "L7jIMkjoD6pT",
        "outputId": "46009215-05a4-4336-dd50-cfcdf9413a47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='430' max='215' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [215/215 14:47]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.650933563709259,\n",
              " 'eval_accuracy': 0.622093023255814,\n",
              " 'eval_precision': 0.6011560693641619,\n",
              " 'eval_recall': 0.7255813953488373,\n",
              " 'eval_f1': 0.6575342465753424,\n",
              " 'eval_runtime': 433.9763,\n",
              " 'eval_samples_per_second': 3.963,\n",
              " 'eval_steps_per_second': 0.495,\n",
              " 'epoch': 2.998256827425915}"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model('./results/PEFT/HEDOS_BERTPrefixTuningCustomModel')"
      ],
      "metadata": {
        "id": "QYXOFxbkDFk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Para 'model' é como instância de BertForSequenceClassification e 'tokenizer'  instância de BertTokenizer\n",
        "model.save_pretrained(\"./results/PEFT/HEDOS_BERTPrefixTuningCustomModel\")\n",
        "tokenizer.save_pretrained(\"./results/PEFT/HEDOS_BERTPrefixTuningCustomModel\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75jSxX_9G8J7",
        "outputId": "87fffe7e-46b9-40cf-8238-91935dc2e0e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./results/PEFT/HEDOS_BERTPrefixTuningCustomModel/tokenizer_config.json',\n",
              " './results/PEFT/HEDOS_BERTPrefixTuningCustomModel/special_tokens_map.json',\n",
              " './results/PEFT/HEDOS_BERTPrefixTuningCustomModel/vocab.txt',\n",
              " './results/PEFT/HEDOS_BERTPrefixTuningCustomModel/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validação"
      ],
      "metadata": {
        "id": "vjtkDNnq70Fu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparar o tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Carregar o modelo\n",
        "model_2 = BertForSequenceClassification.from_pretrained(\"./results/PEFT/HEDOS_BERTPrefixTuningCustomModel\")\n",
        "model_2.to('cuda')"
      ],
      "metadata": {
        "id": "lPJgS894JvbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir o prefixo e configurações de tokenizer se usadas durante o treinamento\n",
        "prefix_text = \"Classifique como tóxico qualquer texto que contenha palavões!\"\n",
        "prefix_tokens = tokenizer(prefix_text, return_tensors=\"pt\", add_special_tokens=False)['input_ids'][0]\n",
        "max_length = 512 - len(prefix_tokens)\n",
        "\n",
        "# Textos de exemplo para avaliação\n",
        "examples = [\n",
        "    \"Você é incrível!\",\n",
        "    \"Eu odeio quando você age assim!\",\n",
        "    \"Esse lugar é horrível e as pessoas são terríveis.\",\n",
        "    \"Que dia maravilhoso para um passeio no parque!\",\n",
        "    \"Você é um idiota e ninguém gosta de você.\",\n",
        "    \"vá para o inferno\",\n",
        "    \"demonho\",\n",
        "    \"puta que pariu, eu amei!\",\n",
        "    \"nordestino nojento\"\n",
        "]\n",
        "\n",
        "# Tokenizar os exemplos\n",
        "#encoded_inputs = tokenizer(examples, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "\n",
        "# Garantir que os inputs estejam no dispositivo CUDA\n",
        "encoded_inputs = {key: val.to('cuda') for key, val in encoded_inputs.items()}\n",
        "\n",
        "# Adicionar prefixo aos inputs\n",
        "device = 'gpu'  # ou 'cpu' se CUDA não estiver disponível\n",
        "batch_prefix_tokens = prefix_tokens.to(device).unsqueeze(0).repeat(encoded_inputs['input_ids'].size(0), 1)\n",
        "extended_input_ids = torch.cat([batch_prefix_tokens, encoded_inputs['input_ids']], dim=1)\n",
        "\n",
        "# Garantir que a attention mask também esteja ajustada\n",
        "prefix_mask = torch.ones(batch_prefix_tokens.size(), dtype=torch.long, device=device)\n",
        "extended_attention_mask = torch.cat([prefix_mask, encoded_inputs['attention_mask']], dim=1)\n",
        "\n",
        "# Obter previsões\n",
        "with torch.no_grad():\n",
        "    outputs = model_2(input_ids=extended_input_ids, attention_mask=extended_attention_mask)\n",
        "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "    probabilities = F.softmax(outputs.logits, dim=-1)  # Softmax sobre os logits para probabilidades\n",
        "\n",
        "# Imprimir os resultados\n",
        "for text, pred, prob in zip(examples, predictions, probabilities):\n",
        "    prob_t = prob[pred].item() * 100  # Probabilidade da classe prevista\n",
        "    print(f\"Sentença: {text} - Tóxico: {'Sim' if pred.item() == 1 else 'Não'} - Probabilidade: {prob_t:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TF4pBa5jPGP",
        "outputId": "d650e482-8ec2-4573-c0c7-4d2ecc749ff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentença: Você é incrível! - Tóxico: Sim - Probabilidade: 66.69%\n",
            "Sentença: Eu odeio quando você age assim! - Tóxico: Sim - Probabilidade: 63.09%\n",
            "Sentença: Esse lugar é horrível e as pessoas são terríveis. - Tóxico: Sim - Probabilidade: 69.35%\n",
            "Sentença: Que dia maravilhoso para um passeio no parque! - Tóxico: Sim - Probabilidade: 61.95%\n",
            "Sentença: Você é um idiota e ninguém gosta de você. - Tóxico: Sim - Probabilidade: 72.66%\n",
            "Sentença: vá para o inferno - Tóxico: Sim - Probabilidade: 65.48%\n",
            "Sentença: demonho - Tóxico: Sim - Probabilidade: 63.66%\n",
            "Sentença: puta que pariu, eu amei! - Tóxico: Sim - Probabilidade: 67.89%\n",
            "Sentença: nordestino nojento - Tóxico: Sim - Probabilidade: 61.39%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERTimbal"
      ],
      "metadata": {
        "id": "8zUzr2-jxKBp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Treino"
      ],
      "metadata": {
        "id": "lCLp0Kxn74Vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqqSoY4THI29",
        "outputId": "0c85b6e3-e2c4-41d2-b9b4-e12799734472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "\n",
        "class BertWithPrefix(torch.nn.Module):\n",
        "    def __init__(self, bert_model, prefix_tokens):\n",
        "        super().__init__()\n",
        "        self.bert = bert_model\n",
        "        self.prefix_tokens = torch.nn.Parameter(prefix_tokens.clone().detach().float())\n",
        "\n",
        "        # Congelar os pesos do modelo BERT, exceto para a camada de classificação\n",
        "        for name, param in self.bert.named_parameters():\n",
        "            if 'classifier' not in name:\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
        "        batch_prefix_tokens = self.prefix_tokens.unsqueeze(0).repeat(input_ids.size(0), 1)\n",
        "        extended_input_ids = torch.cat([batch_prefix_tokens.long(), input_ids], dim=1)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            prefix_mask = torch.ones(batch_prefix_tokens.size(), dtype=torch.long)\n",
        "            extended_attention_mask = torch.cat([prefix_mask, attention_mask], dim=1)\n",
        "        else:\n",
        "            extended_attention_mask = None\n",
        "\n",
        "        outputs = self.bert(input_ids=extended_input_ids, attention_mask=extended_attention_mask, labels=labels)\n",
        "        return outputs\n",
        "\n",
        "# Carregar dados\n",
        "data = pd.read_csv('./repos/HEDOS/HEDOS.csv')\n",
        "filtered_data = data[data['final_label'] != 'Lixo']\n",
        "label_mapping = {'not_toxic': 0, 'toxic': 1}\n",
        "filtered_data['final_label'] = filtered_data['final_label'].map(label_mapping)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(filtered_data['text'], filtered_data['final_label'], test_size=0.2, stratify=filtered_data['final_label'], random_state=42)\n",
        "\n",
        "# Carregar o tokenizer e o modelo BERT pré-treinado para classificação de sentenças\n",
        "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', do_lower_case=False)\n",
        "model = AutoModelForSequenceClassification.from_pretrained('neuralmind/bert-base-portuguese-cased', num_labels=2)\n",
        "\n",
        "# Define and prepare prefix\n",
        "prefix_text = \"Classifique como tóxico qualquer texto que contenha palavões!\"\n",
        "prefix_tokens = tokenizer(prefix_text, return_tensors=\"pt\", add_special_tokens=False)['input_ids'][0]\n",
        "max_length = 512 - len(prefix_tokens)\n",
        "\n",
        "def preprocess_function(examples, labels):\n",
        "    tokenized_inputs = tokenizer(examples, padding=\"max_length\", truncation=True, max_length=max_length)\n",
        "    return {'input_ids': tokenized_inputs['input_ids'], 'attention_mask': tokenized_inputs['attention_mask'], 'labels': labels}\n",
        "\n",
        "train_encodings = preprocess_function(X_train.tolist(), y_train.tolist())\n",
        "val_encodings = preprocess_function(X_val.tolist(), y_val.tolist())\n",
        "\n",
        "train_dataset = Dataset.from_dict(train_encodings)\n",
        "eval_dataset = Dataset.from_dict(val_encodings)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy='epoch',\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    fp16=False  # Desativar FP16 para execução em CPU\n",
        ")\n",
        "\n",
        "def compute_metrics(p):\n",
        "    pred, labels = p.predictions, p.label_ids\n",
        "    pred = np.argmax(pred, axis=1)\n",
        "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
        "    recall = recall_score(y_true=labels, y_pred=pred)\n",
        "    precision = precision_score(y_true=labels, y_pred=pred)\n",
        "    f1 = f1_score(y_true=labels, y_pred=pred)\n",
        "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "\n",
        "bert_with_prefix = BertWithPrefix(model, prefix_tokens)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=bert_with_prefix,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Iniciar o treinamento\n",
        "try:\n",
        "    train_result = trainer.train()\n",
        "    eval_result = trainer.evaluate()\n",
        "    results_dict = {\n",
        "        \"training_loss\": train_result.training_loss,\n",
        "        \"eval_accuracy\": eval_result['eval_accuracy'],\n",
        "        \"eval_f1\": eval_result['eval_f1'],\n",
        "        \"train_runtime\": train_result.metrics['train_runtime'],\n",
        "        \"train_samples_per_second\": train_result.metrics['train_samples_per_second'],\n",
        "        \"total_flos\": train_result.metrics['total_flos']\n",
        "    }\n",
        "    with open('./results/PEFT/hedos_bert_prefixtuning_training_results.json', 'w') as f:\n",
        "        json.dump(results_dict, f)\n",
        "    print(\"Resultados salvos com sucesso!\")\n",
        "except Exception as e:\n",
        "    print(f\"Erro durante o treinamento ou avaliação: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "6TBWzXnKaF4A",
        "outputId": "b593e861-f94b-436e-9e3c-6d84567ce5b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_35585/815284559.py:36: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_data['final_label'] = filtered_data['final_label'].map(label_mapping)\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/home/sarapinheiro/.local/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='411' max='411' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [411/411 1:48:31, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.568000</td>\n",
              "      <td>0.525624</td>\n",
              "      <td>0.781620</td>\n",
              "      <td>0.786175</td>\n",
              "      <td>0.990708</td>\n",
              "      <td>0.876670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.494500</td>\n",
              "      <td>0.516628</td>\n",
              "      <td>0.783439</td>\n",
              "      <td>0.783439</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.878571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.532800</td>\n",
              "      <td>0.515868</td>\n",
              "      <td>0.783439</td>\n",
              "      <td>0.783439</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.878571</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='138' max='138' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [138/138 07:18]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados salvos com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "z2MQJfffHW9z",
        "outputId": "75f69fbb-1d56-452e-a254-96069e25690c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='276' max='138' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [138/138 16:28]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.5158679485321045,\n",
              " 'eval_accuracy': 0.7834394904458599,\n",
              " 'eval_precision': 0.7834394904458599,\n",
              " 'eval_recall': 1.0,\n",
              " 'eval_f1': 0.8785714285714286,\n",
              " 'eval_runtime': 438.4244,\n",
              " 'eval_samples_per_second': 2.507,\n",
              " 'eval_steps_per_second': 0.315,\n",
              " 'epoch': 2.994535519125683}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model('./results/PEFT/HEDOS_BERTimbauPrefixTuningCustomModel')"
      ],
      "metadata": {
        "id": "q1qBQenRHykD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Supondo que 'model' é sua instância de BertForSequenceClassification e 'tokenizer' é sua instância de BertTokenizer\n",
        "model.save_pretrained(\"./results/PEFT/HEDOS_BERTimbauPrefixTuningCustomModel\")\n",
        "tokenizer.save_pretrained(\"./results/PEFT/HEDOS_BERTimbauPrefixTuningCustomModel\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tscOUMaiGgnO",
        "outputId": "4d579e7f-34bf-4344-b385-6b075bbd7005"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./results/PEFT/HEDOS_BERTimbauPrefixTuningCustomModel/tokenizer_config.json',\n",
              " './results/PEFT/HEDOS_BERTimbauPrefixTuningCustomModel/special_tokens_map.json',\n",
              " './results/PEFT/HEDOS_BERTimbauPrefixTuningCustomModel/vocab.txt',\n",
              " './results/PEFT/HEDOS_BERTimbauPrefixTuningCustomModel/added_tokens.json',\n",
              " './results/PEFT/HEDOS_BERTimbauPrefixTuningCustomModel/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validação"
      ],
      "metadata": {
        "id": "1gBtgzQM8AXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar o tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "\n",
        "# Carregar o modelo\n",
        "model_2 = BertForSequenceClassification.from_pretrained(\"./results/PEFT/HEDOS_BERTimbauPrefixTuningCustomModel\")\n",
        "#model_2.to('cuda')"
      ],
      "metadata": {
        "id": "0rz1ixl1H1vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir o prefixo e configurações de tokenizer se usadas durante o treinamento\n",
        "prefix_text = \"Classifique como tóxico qualquer texto que contenha palavões!\"\n",
        "prefix_tokens = tokenizer(prefix_text, return_tensors=\"pt\", add_special_tokens=False)['input_ids'][0]\n",
        "max_length = 512 - len(prefix_tokens)  # Ajuste conforme sua configuração\n",
        "\n",
        "# Textos de exemplo para avaliação\n",
        "examples = [\n",
        "    \"Você é incrível!\",\n",
        "    \"Eu odeio quando você age assim!\",\n",
        "    \"Esse lugar é horrível e as pessoas são terríveis.\",\n",
        "    \"Que dia maravilhoso para um passeio no parque!\",\n",
        "    \"Você é um idiota e ninguém gosta de você.\",\n",
        "    \"vá para o inferno\",\n",
        "    \"demonho\",\n",
        "    \"puta que pariu, eu amei!\",\n",
        "    \"nordestino nojento\"\n",
        "]\n",
        "\n",
        "# Tokenizar os exemplos\n",
        "encoded_inputs = tokenizer(examples, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "\n",
        "# Garantir que os inputs estejam no dispositivo CUDA\n",
        "#encoded_inputs = {key: val.to('cuda') for key, val in encoded_inputs.items()}\n",
        "\n",
        "# Adicionar prefixo aos inputs\n",
        "device = 'cpu'  # ou 'cpu' se CUDA não estiver disponível\n",
        "batch_prefix_tokens = prefix_tokens.to(device).unsqueeze(0).repeat(encoded_inputs['input_ids'].size(0), 1)\n",
        "extended_input_ids = torch.cat([batch_prefix_tokens, encoded_inputs['input_ids']], dim=1)\n",
        "\n",
        "# Garantir que a attention mask também esteja ajustada\n",
        "prefix_mask = torch.ones(batch_prefix_tokens.size(), dtype=torch.long, device=device)\n",
        "extended_attention_mask = torch.cat([prefix_mask, encoded_inputs['attention_mask']], dim=1)\n",
        "\n",
        "# Obter previsões\n",
        "with torch.no_grad():\n",
        "    outputs = model_2(input_ids=extended_input_ids, attention_mask=extended_attention_mask)\n",
        "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "    probabilities = F.softmax(outputs.logits, dim=-1)  # Softmax sobre os logits para probabilidades\n",
        "\n",
        "# Imprimir os resultados\n",
        "for text, pred, prob in zip(examples, predictions, probabilities):\n",
        "    prob_t = prob[pred].item() * 100  # Probabilidade da classe prevista\n",
        "    print(f\"Sentença: {text} - Tóxico: {'Sim' if pred.item() == 1 else 'Não'} - Probabilidade: {prob_t:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hEjh0AkpuU1",
        "outputId": "a24aea33-f332-4c5d-c6b3-95d59ac26370"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentença: Você é incrível! - Tóxico: Sim - Probabilidade: 54.90%\n",
            "Sentença: Eu odeio quando você age assim! - Tóxico: Não - Probabilidade: 51.38%\n",
            "Sentença: Esse lugar é horrível e as pessoas são terríveis. - Tóxico: Não - Probabilidade: 61.07%\n",
            "Sentença: Que dia maravilhoso para um passeio no parque! - Tóxico: Sim - Probabilidade: 55.12%\n",
            "Sentença: Você é um idiota e ninguém gosta de você. - Tóxico: Não - Probabilidade: 50.97%\n",
            "Sentença: vá para o inferno - Tóxico: Sim - Probabilidade: 80.83%\n",
            "Sentença: demonho - Tóxico: Sim - Probabilidade: 80.37%\n",
            "Sentença: puta que pariu, eu amei! - Tóxico: Sim - Probabilidade: 81.92%\n",
            "Sentença: nordestino nojento - Tóxico: Sim - Probabilidade: 80.91%\n"
          ]
        }
      ]
    }
  ]
}